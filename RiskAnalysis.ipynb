# ============================================================
# SAP FINANCIAL DOCUMENT – FRAUD / ANOMALY DETECTION PIPELINE
# ============================================================

import pandas as pd
import numpy as np

from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier

# ============================================================
# 1. LOAD DATA
# ============================================================

df = pd.read_excel("SAP_DataFraud_Full.xlsx")

# Clean and standardize columns
df.columns = [c.strip().replace(" ", "_") for c in df.columns]

# ============================================================
# 2. DATE CLEANING (SAP-style dates)
# ============================================================

# Replace SAP null datetime "00.01.1900" → NaN
df['Time_Stamp_of_Last_Change'] = df['Time_Stamp_of_Last_Change'].replace(
    r"00\.01\.1900.*", np.nan, regex=True
)

# Columns containing dates
date_cols = ['Document_Date', 'Posting_Date', 'Entry_Date', 'Time_Stamp_of_Last_Change']

# Convert all with dayfirst=True to avoid warnings
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)

# ============================================================
# 3. TIME CLEANING
# ============================================================

def parse_time(t):
    if pd.isna(t):
        return np.nan
    return pd.to_datetime(t, format="%H:%M:%S", errors='coerce').time()

df['Entry_Time_clean'] = df['Time_of_Entry'].apply(parse_time)

# ============================================================
# 4. AMOUNT CLEANING
# ============================================================

df['Amount_in_Loc._Curr.'] = (
    df['Amount_in_Loc._Curr.'].astype(str)
        .str.replace(",", "", regex=False)
        .astype(float)
)

df['Amount_Abs'] = df['Amount_in_Loc._Curr.'].abs()

# ============================================================
# 5. DEBIT / CREDIT
# ============================================================

df['Debit_Credit_clean'] = df['Debit/Credit_Ind.'].astype(str).str.upper().str.strip()
df['Is_Debit'] = df['Debit_Credit_clean'].map({'D': 1, 'C': 0, 'S': 0}).fillna(0)

# ============================================================
# 6. LAST CHANGE DATE DIFFERENCE
# ============================================================

df['Time_Stamp_of_Last_Change'] = df['Time_Stamp_of_Last_Change'].fillna(df['Posting_Date'])

df['Days_Between_LastChange'] = (
    df['Time_Stamp_of_Last_Change'] - df['Posting_Date']
).dt.days

# ============================================================
# 7. FEATURE ENGINEERING
# ============================================================

df['Entry_vs_Posting_Days'] = (df['Posting_Date'] - df['Entry_Date']).dt.days

df['Backdated_Flag'] = (df['Document_Date'] < df['Posting_Date']).astype(int)

df['Changed_After_Posting_Flag'] = (df['Days_Between_LastChange'] > 0).astype(int)

df['Posting_Hour'] = df['Entry_Time_clean'].apply(lambda t: t.hour if pd.notna(t) else 0)

df['Posting_DayOfWeek'] = df['Posting_Date'].dt.day_name()
df['Is_Weekend'] = df['Posting_DayOfWeek'].isin(['Saturday', 'Sunday']).astype(int)

df['Is_After_Hours'] = df['Posting_Hour'].apply(lambda h: 1 if (h < 8 or h > 20) else 0)

# Daily posting volume by user
df['Daily_Post_Count_by_User'] = (
    df.groupby(['User_Name', 'Posting_Date'])['Document_Number'].transform('count')
)

# Rare Document Types & G/L Accounts
doc_freq = df['Document_Type'].value_counts(normalize=True)
gl_freq = df['G/L_Account'].value_counts(normalize=True)

df['User_Rare_DocType_Flag'] = df['Document_Type'].map(lambda x: 1 if doc_freq[x] < 0.05 else 0)
df['User_Rare_GL_Flag'] = df['G/L_Account'].map(lambda x: 1 if gl_freq[x] < 0.03 else 0)

# Manual posting keys
df['PostingKey_Manual_Flag'] = df['Posting_Key'].astype(str).isin(['40', '50', '01', '11']).astype(int)

# Duplicate flags
df['Duplicate_Reference_Flag'] = df.duplicated(
    ['Company_Code', 'Reference_Key'], keep=False
).astype(int)

df['Duplicate_GL_Amount_Flag'] = df.duplicated(
    ['Company_Code', 'G/L_Account', 'Amount_in_Loc._Curr.'], keep=False
).astype(int)

# High value transactions
df['High_Value_Flag'] = (
    df['Amount_Abs'] > df['Amount_Abs'].mean() + 3 * df['Amount_Abs'].std()
).astype(int)

# ============================================================
# 8. UNSUPERVISED ANOMALY DETECTION – ISOLATION FOREST
# ============================================================

features = [
    'Entry_vs_Posting_Days', 'Backdated_Flag', 'Days_Between_LastChange',
    'Changed_After_Posting_Flag', 'Posting_Hour', 'Is_Weekend', 'Is_After_Hours',
    'Daily_Post_Count_by_User', 'User_Rare_DocType_Flag', 'User_Rare_GL_Flag',
    'Amount_Abs', 'High_Value_Flag', 'Is_Debit', 'PostingKey_Manual_Flag',
    'Duplicate_Reference_Flag', 'Duplicate_GL_Amount_Flag'
]

df_model = df[features].fillna(0)

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(df_model)

iso = IsolationForest(n_estimators=300, contamination=0.10, random_state=42)
iso.fit(X_scaled)

df['Fraud_Risk'] = MinMaxScaler().fit_transform(
    (-iso.decision_function(X_scaled)).reshape(-1, 1)
)

df['Fraud_Anomaly_Flag'] = (iso.predict(X_scaled) == -1).astype(int)

# ============================================================
# 9. SEMI-SUPERVISED LEARNING (PSEUDO LABELS)
# ============================================================

df['Pseudo_Fraud'] = (df['Fraud_Risk'] > 0.75).astype(int)

# ============================================================
# 10. SUPERVISED MODELS (RF, LR, XGB)
# ============================================================

imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(df_model)

X_scaled2 = MinMaxScaler().fit_transform(X_imp)

# Random Forest
rf = RandomForestClassifier(n_estimators=300, random_state=42)
rf.fit(X_scaled2, df['Pseudo_Fraud'])
df['RF_Prob'] = rf.predict_proba(X_scaled2)[:, 1]

# Logistic Regression
lr = LogisticRegression(max_iter=300)
lr.fit(X_scaled2, df['Pseudo_Fraud'])
df['LR_Prob'] = lr.predict_proba(X_scaled2)[:, 1]

# XGBoost
xgb = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1, random_state=42)
xgb.fit(X_scaled2, df['Pseudo_Fraud'])
df['XGB_Prob'] = xgb.predict_proba(X_scaled2)[:, 1]

# ============================================================
# 11. FINAL BLENDED FRAUD SCORE
# ============================================================

df['Final_Fraud_Score'] = (
    0.40 * df['Fraud_Risk'] +
    0.30 * df['RF_Prob'] +
    0.20 * df['XGB_Prob'] +
    0.10 * df['LR_Prob']
)

df['Final_Fraud_Score'] = MinMaxScaler().fit_transform(
    df['Final_Fraud_Score'].values.reshape(-1, 1)
)

# ============================================================
# 12. EXPORT RESULTS
# ============================================================

df.to_csv("fraud_semi_supervised_scored.csv", index=False)

print("✔️ Fully optimized SAP fraud modeling pipeline completed!")
